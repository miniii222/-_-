{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "\n",
    "from time import time\n",
    "import datetime\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse, mod\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "train_label = train['target']\n",
    "train_id = train['id']\n",
    "\n",
    "del train['target'], train['id']\n",
    "\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "test_id = test['id']\n",
    "del test['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기초 통계 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_num_on_cat(train_df, test_df, target_column, group_column):\n",
    "    # train_df : 훈련 데이터\n",
    "    # test_df : 테스트 데이터\n",
    "    # target_column : 통계기반 파생 변수를 생성한 타겟 변수\n",
    "    # group_column : 피봇(pivot)을 수행할 변수\n",
    "    train_df['row_id'] = range(train_df.shape[0])\n",
    "    test_df['row_id'] = range(test_df.shape[0])\n",
    "    train_df['train'] = 1\n",
    "    test_df['train'] = 0\n",
    "\n",
    "    # 훈련 데이터와 테스트 데이터를 통합한다\n",
    "    all_df = train_df[['row_id', 'train', target_column, group_column]].append(test_df[['row_id','train', target_column, group_column]])\n",
    "    \n",
    "    # group_column 기반으로 피봇한 target_column의 값을 구한다 \n",
    "    grouped = all_df[[target_column, group_column]].groupby(group_column)\n",
    "\n",
    "    # 빈도(size), 평균(mean), 표준편차(std), 중간값(median), 최대값(max), 최소값(min)을 구한다\n",
    "    the_size = pd.DataFrame(grouped.size()).reset_index()\n",
    "    the_size.columns = [group_column, '%s_size' % target_column]\n",
    "    the_mean = pd.DataFrame(grouped.mean()).reset_index()\n",
    "    the_mean.columns = [group_column, '%s_mean' % target_column]\n",
    "    the_std = pd.DataFrame(grouped.std()).reset_index().fillna(0)\n",
    "    the_std.columns = [group_column, '%s_std' % target_column]\n",
    "    the_median = pd.DataFrame(grouped.median()).reset_index()\n",
    "    the_median.columns = [group_column, '%s_median' % target_column]\n",
    "    the_max = pd.DataFrame(grouped.max()).reset_index()\n",
    "    the_max.columns = [group_column, '%s_max' % target_column]\n",
    "    the_min = pd.DataFrame(grouped.min()).reset_index()\n",
    "    the_min.columns = [group_column, '%s_min' % target_column]\n",
    "\n",
    "    # 통계 기반 파생 변수를 취합한다\n",
    "    the_stats = pd.merge(the_size, the_mean)\n",
    "    the_stats = pd.merge(the_stats, the_std)\n",
    "    the_stats = pd.merge(the_stats, the_median)\n",
    "    the_stats = pd.merge(the_stats, the_max)\n",
    "    the_stats = pd.merge(the_stats, the_min)\n",
    "    all_df = pd.merge(all_df, the_stats, how='left')\n",
    "\n",
    "    # 훈련 데이터와 테스트 데이터로 분리하여 반환한다\n",
    "    selected_train = all_df[all_df['train'] == 1]\n",
    "    selected_test = all_df[all_df['train'] == 0]\n",
    "    selected_train.sort_values('row_id', inplace=True)\n",
    "    selected_test.sort_values('row_id', inplace=True)\n",
    "    selected_train.drop([target_column, group_column, 'row_id', 'train'], axis=1, inplace=True)\n",
    "    selected_test.drop([target_column, group_column, 'row_id', 'train'], axis=1, inplace=True)\n",
    "    selected_train, selected_test = np.array(selected_train), np.array(selected_test)\n",
    "    return selected_train, selected_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 간의 다양한 상호 작용 파생 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_features(train, test, fea1, fea2, prefix):\n",
    "    # train : 훈련 데이터\n",
    "    # test : 테스트 데이터\n",
    "    # fea1, fea2 : 상호 작용을 수행할 변수 이름\n",
    "    # prefix : 파생 변수의 변수 이름\n",
    "\n",
    "    # 두 변수간의 곱셈/나눗셈 상호 작용에 대한 파생 변수를 생성한다\n",
    "    train['inter_{}*'.format(prefix)] = train[fea1] * train[fea2]\n",
    "    train['inter_{}/'.format(prefix)] = train[fea1] / train[fea2]\n",
    "\n",
    "    test['inter_{}*'.format(prefix)] = test[fea1] * test[fea2]\n",
    "    test['inter_{}/'.format(prefix)] = test[fea1] / test[fea2]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# 범주형 변수와 이진 변수 이름을 추출한다\n",
    "cat_fea = [x for x in list(train) if 'cat' in x]\n",
    "bin_fea = [x for x in list(train) if 'bin' in x]\n",
    "\n",
    "# 결측값 (-1)의 개수로 missing 파생 변수를 생성한다\n",
    "train['missing'] = (train==-1).sum(axis=1).astype(float)\n",
    "test['missing'] = (test==-1).sum(axis=1).astype(float)\n",
    "\n",
    "# 6개 변수에 대하여 상호작용 변수를 생성한다\n",
    "for e, (x, y) in enumerate(combinations(['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01'], 2)):\n",
    "    train, test = interaction_features(train, test, x, y, e)\n",
    "\n",
    "# 수치형 변수, 상호 작용 파생 변수, ind 변수 이름을 추출한다\n",
    "num_features = [c for c in list(train) if ('cat' not in c and 'calc' not in c)]\n",
    "num_features.append('missing')\n",
    "inter_fea = [x for x in list(train) if 'inter' in x]\n",
    "feature_names = list(train)\n",
    "ind_features = [c for c in feature_names if 'ind' in c]\n",
    "\n",
    "# ind 변수 그룹의 조합을 하나의 문자열 변수로 표현한다\n",
    "count = 0\n",
    "for c in ind_features:\n",
    "    if count == 0:\n",
    "        train['new_ind'] = train[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        train['new_ind'] += '_' + train[c].astype(str)\n",
    "ind_features = [c for c in feature_names if 'ind' in c]\n",
    "count = 0\n",
    "for c in ind_features:\n",
    "    if count == 0:\n",
    "        test['new_ind'] = test[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        test['new_ind'] += '_' + test[c].astype(str)\n",
    "\n",
    "# reg 변수 그룹의 조합을 하나의 문자열 변수로 표현한다\n",
    "reg_features = [c for c in feature_names if 'reg' in c]\n",
    "count = 0\n",
    "for c in reg_features:\n",
    "    if count == 0:\n",
    "        train['new_reg'] = train[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        train['new_reg'] += '_' + train[c].astype(str)\n",
    "reg_features = [c for c in feature_names if 'reg' in c]\n",
    "count = 0\n",
    "for c in reg_features:\n",
    "    if count == 0:\n",
    "        test['new_reg'] = test[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        test['new_reg'] += '_' + test[c].astype(str)\n",
    "\n",
    "# car 변수 그룹의 조합을 하나의 문자열 변수로 표현한다\n",
    "car_features = [c for c in feature_names if 'car' in c]\n",
    "count = 0\n",
    "for c in car_features:\n",
    "    if count == 0:\n",
    "        train['new_car'] = train[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        train['new_car'] += '_' + train[c].astype(str)\n",
    "car_features = [c for c in feature_names if 'car' in c]\n",
    "count = 0\n",
    "for c in car_features:\n",
    "    if count == 0:\n",
    "        test['new_car'] = test[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        test['new_car'] += '_' + test[c].astype(str)\n",
    "\n",
    "# 범주형 데이터와 수치형 데이터를 따로 관리한다\n",
    "train_cat = train[cat_fea]\n",
    "train_num = train[[x for x in list(train) if x in num_features]]\n",
    "test_cat = test[cat_fea]\n",
    "test_num = test[[x for x in list(train) if x in num_features]]\n",
    "\n",
    "# 범주형 데이터에 LabelEncode()를 수행한다\n",
    "max_cat_values = []\n",
    "for c in cat_fea:\n",
    "    le = LabelEncoder()\n",
    "    x = le.fit_transform(pd.concat([train_cat, test_cat])[c])\n",
    "    train_cat[c] = le.transform(train_cat[c])\n",
    "    test_cat[c] = le.transform(test_cat[c])\n",
    "    max_cat_values.append(np.max(x))\n",
    "\n",
    "# 범주형 변수의 빈도값으로 새로운 파생 변수를 생성한다\n",
    "cat_count_features = []\n",
    "for c in cat_fea + ['new_ind','new_reg','new_car']:\n",
    "    d = pd.concat([train[c],test[c]]).value_counts().to_dict()\n",
    "    train['%s_count'%c] = train[c].apply(lambda x:d.get(x,0))\n",
    "    test['%s_count'%c] = test[c].apply(lambda x:d.get(x,0))\n",
    "    cat_count_features.append('%s_count'%c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import Xgboost features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea0, test_fea0 = pickle.load(open(\"./data/fea0.pk\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# 수치형 변수의 결측값/이상값을 0으로 대체하고, 범주형 변수와 XGBoost 기반 변수를 통합한다\n",
    "train_list = [train_num.replace([np.inf, -np.inf, np.nan], 0), train[cat_count_features], train_fea0]\n",
    "test_list = [test_num.replace([np.inf, -np.inf, np.nan], 0), test[cat_count_features], test_fea0]\n",
    "\n",
    "# 피봇 기반 기초 통계 파생 변수를 생성한다\n",
    "for t in ['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01']:\n",
    "    for g in ['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01', 'ps_ind_05_cat']:\n",
    "        if t != g:\n",
    "            # group_column 변수를 기반으로 target_column 값을 피봇한 후, 기초 통계 값을 파생 변수로 추가한다\n",
    "            s_train, s_test = proj_num_on_cat(train, test, target_column=t, group_column=g)\n",
    "            train_list.append(s_train)\n",
    "            test_list.append(s_test)\n",
    "            \n",
    "# 데이터 전체를 메모리 효율성을 위하여 희소행렬로 변환한다\n",
    "X = sparse.hstack(train_list).tocsr()\n",
    "X_test = sparse.hstack(test_list).tocsr()\n",
    "all_data = np.vstack([X.toarray(), X_test.toarray()])\n",
    "\n",
    "# 인공신경망 학습을 위해 모든 변수값을 -1~1로 Scaling한다\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_data)\n",
    "X = scaler.transform(X.toarray())\n",
    "X_test = scaler.transform(X_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 위의 피쳐들만 이용해서 조합을 만들었는지는 알 수 없다 ㅜㅡㅜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2계층 인공 신경망 모델을 정의한다\n",
    "def nn_model():\n",
    "    inputs = []\n",
    "    flatten_layers = []\n",
    "\n",
    "    # 범주형 변수에 대한 Embedding 계층을 정의한다. 모든 범주형 변수는 해당 변수의 최대값(num_c) 크기의 벡터 임베딩을 학습한다.\n",
    "    for e, c in enumerate(cat_fea):\n",
    "        input_c = Input(shape=(1, ), dtype='int32')\n",
    "        num_c = max_cat_values[e]\n",
    "        embed_c = Embedding(\n",
    "            num_c,\n",
    "            6,\n",
    "            input_length=1\n",
    "        )(input_c)\n",
    "        embed_c = Dropout(0.25)(embed_c)\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "\n",
    "        inputs.append(input_c)\n",
    "        flatten_layers.append(flatten_c)\n",
    "\n",
    "    # 수치형 변수에 대한 입력 계층을 정의한다\n",
    "    input_num = Input(shape=(X.shape[1],), dtype='float32')\n",
    "    flatten_layers.append(input_num)\n",
    "    inputs.append(input_num)\n",
    "\n",
    "    # 범주형 변수와 수치형 변수를 통합하여 2계층 Fully Connected Layer를 정의한다\n",
    "    flatten = merge(flatten_layers, mode='concat')\n",
    "\n",
    "    # 1계층은 512 차원을 가지며, PReLU Activation 함수와 BatchNormalization, Dropout 함수를 통과한다\n",
    "    fc1 = Dense(512, init='he_normal')(flatten)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.75)(fc1)\n",
    "\n",
    "    # 2계층은 64 차원을 가진다\n",
    "    fc1 = Dense(64, init='he_normal')(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.5)(fc1)\n",
    "\n",
    "    outputs = Dense(1, init='he_normal', activation='sigmoid')(fc1)\n",
    "\n",
    "    # 모델 학습을 수행하는 optimizer와 학습 기준이 되는 loss 함수를 정의한다\n",
    "    model = Model(input = inputs, output = outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-70902f9a5190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# 인공 신경망 모델을 정의한다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m# 모델을 학습한다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtr_cat_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxte_cat_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-9b9ed85f277c>\u001b[0m in \u001b[0;36mnn_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 범주형 변수와 수치형 변수를 통합하여 2계층 Fully Connected Layer를 정의한다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# 1계층은 512 차원을 가지며, PReLU Activation 함수와 BatchNormalization, Dropout 함수를 통과한다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# 5-Fold 교차 검증을 수행한다\n",
    "NFOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "# 모델 학습을 5번의 랜덤 시드로 수행한 후, 평균값을 최종 결과로 얻는다\n",
    "num_seeds = 5\n",
    "begintime = time()\n",
    "\n",
    "# 내부 교차 검증 및 테스트 데이터에 대한 예측값을 저장하기 위한 준비를 한다\n",
    "cv_train = np.zeros(len(train_label))\n",
    "cv_pred = np.zeros(len(test_id))\n",
    "\n",
    "X_cat = train_cat.as_matrix()\n",
    "X_test_cat = test_cat.as_matrix()\n",
    "\n",
    "x_test_cat = []\n",
    "for i in range(X_test_cat.shape[1]):\n",
    "    x_test_cat.append(X_test_cat[:, i].reshape(-1, 1))\n",
    "x_test_cat.append(X_test)\n",
    "\n",
    "# 랜덤 시드 개수만큼 모델 학습을 수행한다\n",
    "for s in range(num_seeds):\n",
    "    np.random.seed(s)\n",
    "    for (inTr, inTe) in kfold.split(X, train_label):\n",
    "        xtr = X[inTr]\n",
    "        ytr = train_label[inTr]\n",
    "        xte = X[inTe]\n",
    "        yte = train_label[inTe]\n",
    "\n",
    "        xtr_cat = X_cat[inTr]\n",
    "        xte_cat = X_cat[inTe]\n",
    "\n",
    "        # 범주형 데이터를 추출하여, 수치형 데이터와 통합한다\n",
    "        xtr_cat_list, xte_cat_list = [], []\n",
    "        for i in range(xtr_cat.shape[1]):\n",
    "            xtr_cat_list.append(xtr_cat[:, i].reshape(-1, 1))\n",
    "            xte_cat_list.append(xte_cat[:, i].reshape(-1, 1))\n",
    "        xtr_cat_list.append(xtr)\n",
    "        xte_cat_list.append(xte)\n",
    "\n",
    "        # 인공 신경망 모델을 정의한다\n",
    "        model = nn_model()\n",
    "        # 모델을 학습한다\n",
    "        model.fit(xtr_cat_list, ytr, epochs=20, batch_size=512, verbose=2, validation_data=[xte_cat_list, yte])\n",
    "        \n",
    "        # 예측값의 순위를 구하는 함수 get_rank()를 정의한다. Gini 평가 함수는 예측값 간의 순위를 기준으로 평가하기 때문에 최종 평가 점수에 영향을 미치지 않는다.\n",
    "        def get_rank(x):\n",
    "            return pd.Series(x).rank(pct=True).values\n",
    "        \n",
    "        # 내부 교차 검증 데이터에 대한 예측값을 저장한다\n",
    "        cv_train[inTe] += get_rank(model.predict(x=xte_cat_list, batch_size=512, verbose=0)[:, 0])\n",
    "        print(Gini(train_label[inTe], cv_train[inTe]))\n",
    "        \n",
    "        # 테스트 데이터에 대한 예측값을 저장한다\n",
    "        cv_pred += get_rank(model.predict(x=x_test_cat, batch_size=512, verbose=0)[:, 0])\n",
    "\n",
    "    print(Gini(train_label, cv_train / (1. * (s + 1))))\n",
    "    print(str(datetime.timedelta(seconds=time() - begintime)))\n",
    "    \n",
    "# 테스트 데이터에 대한 최종 예측값을 파일로 저장한다\n",
    "pd.DataFrame({'id': test_id, 'target': get_rank(cv_pred * 1./ (NFOLDS * num_seeds))}).to_csv('../model/keras5_pred.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
